# Grid world parameters
GRID_SIZE = 5
GOAL_POS = (2, 3)
FORBIDDEN_CELLS = [(1, 1), (1, 3), (1, 4), (2, 1), (2, 2), (3, 3)]

# GRID_SIZE = 3
# GOAL_POS = (2, 2)
# FORBIDDEN_CELLS = [(0, 2), (2, 1)]

# Reward parameters
REWARD_TARGET = 1
REWARD_BOUNDARY = -1
REWARD_FORBIDDEN = -10
REWARD_STEP = 0

# RL parameters
SHOW_GRID_WORLD = True

# Value Iteration parameters
VALUE_ITERATION_DISCOUNT_FACTOR = 0.9
VALUE_ITERATION_THRESHOLD = 1e-4
VALUE_ITERATION_MAX_ITERATE_STEPS = 100

# (Truncated) Policy Iteration parameters
# You can set POLICY_EVALUATION_STEPS to get truncated policy iteration
POLICY_ITERATION_DISCOUNT_FACTOR = 0.9
POLICY_EVALUATION_THRESHOLD = 1e-4
POLICY_EVALUATION_STEPS = 20
POLICY_IMPROVEMENT_STEPS = 10

# Monte Carlo parameters
# MC Basic parameters
MC_BASIC_DISCOUNT_FACTOR = 0.9
MC_VALUE_ESTIMATION_THRESHOLD = 1e-4
MC_VALUE_ESTIMATION_MAX_ITERATE_STEPS = 1000
MC_BASIC_EPISODES = 1    # visit each state-action pair once because of deterministic env and policy
MC_BASIC_EPISODE_LENGTH = 20

# MC Exploring Starts parameters
MC_ES_DISCOUNT_FACTOR = 0.9
MC_ES_EPISODES = 10000
MC_ES_EPISODE_LENGTH = 200